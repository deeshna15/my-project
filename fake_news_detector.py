# -*- coding: utf-8 -*-
"""Fake News Detection Model using TensorFlow in Python.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ipkase5LUC3NyZp35LkYYBKA4QrIWRux
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import pickle
data = pd.read_csv("news.csv")
data.head()

data = data.drop(["Unnamed: 0"], axis=1)
data.head(5)

le = preprocessing.LabelEncoder()
le.fit(data['label'])
data['label'] = le.transform(data['label'])

embedding_dim = 50
max_length = 54
padding_type = 'post'
trunc_type = 'post'
oov_tok = "<OOV>"
training_size = 3000
test_portion = 0.1

titles = data['title'][:training_size]
labels = data['label'][:training_size]

tokenizer = Tokenizer(oov_token=oov_tok)
tokenizer.fit_on_texts(titles)
word_index = tokenizer.word_index
vocab_size = len(word_index)

sequences = tokenizer.texts_to_sequences(titles)
padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)

split_index = int(test_portion * training_size)
train_padded = padded[split_index:]
test_padded = padded[:split_index]
train_labels = np.array(labels[split_index:])
test_labels = np.array(labels[:split_index])

embedding_index = {}
with open('glove.6B.50d.txt', 'r', encoding='utf-8') as f:
    for line in f:
        values = line.split()
        word = values[0]
        coefs = np.asarray(values[1:], dtype='float32')
        embedding_index[word] = coefs

embedding_matrix = np.zeros((vocab_size + 1, embedding_dim))
for word, i in word_index.items():
    if i < vocab_size:
        embedding_vector = embedding_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[i] = embedding_vector

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=vocab_size + 1, output_dim=embedding_dim,
                              input_length=max_length, weights=[embedding_matrix], trainable=False),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Conv1D(64, 5, activation='relu'),
    tf.keras.layers.MaxPooling1D(pool_size=4),
    tf.keras.layers.LSTM(64),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()

print(type(train_padded))   # Should print: <class 'numpy.ndarray'>
print(type(train_labels))   # Should print: <class 'numpy.ndarray'>

history = model.fit(
    train_padded,       # ✅ NumPy array
    train_labels,       # ✅ NumPy array
    epochs=50,
    validation_data=(test_padded, test_labels),  # ✅ Both are NumPy arrays
    verbose=2
)
# Save the trained model to an H5 file
model.save('fake_news_model.h5')

# Save the tokenizer to a pickle file
with open('tokenizer.pkl', 'wb') as handle:
    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)

print("Model and tokenizer saved successfully.")

# Use the same tokenizer used during training
'''X = "Karry to go to France in gesture of sympathy"

# Convert to sequence
sequence = tokenizer.texts_to_sequences([X])
padded_sequence = pad_sequences(sequence, maxlen=max_length, padding=padding_type, truncating=trunc_type)

# Predict
prediction = model.predict(padded_sequence, verbose=0)

# Show result
if prediction[0][0] >= 0.5:
    print("This news is True")
else:
    print("This news is False")'''